---
title: "NYPD Shooting Incidents"
author: "Alex"
date: "2025-05-08"
output:
  html_document: default
  pdf_document: default
---

## Importing data

```{r load_libraries, message=FALSE}
library(tidyverse)
```

```{r import_data, message=FALSE}
NYPD_shooting <- read_csv("https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD")
head(NYPD_shooting,10)
```

## Cleaning data

### Removing columns

At first glance, I see some rows that do not seem that useful. These rows being:

-   INCIDENT_KEY
-   Precinct
-   Jurisdiction Code

```{r remove_unused_cols, message=FALSE}
NYPD_shooting <- NYPD_shooting %>%
  select(-INCIDENT_KEY, -PRECINCT, -JURISDICTION_CODE)
colnames(NYPD_shooting)
```

### Handling missing data

This shows the count of missing data within the dataset.

```{r show_missing_data, message=FALSE}
colSums(is.na(NYPD_shooting))
```

We can see that there are a lot missing from `LOC_OF_OCCUR_DESC` and `LOC_CLASSFCTN_DESC`

Seeing that there are only 29,734 rows in the dataset and both of those columns have 25,596 missing rows, it might be best to either just leave the column or delete it. This being the case, both `LOC_OF_OCCUR_DESC` and `LOC_CLASSFCTN_DESC` probably is not an important column so I conclude that I will delete the columns.

```{r remove_na_cols, message=FALSE}
NYPD_shooting <- NYPD_shooting %>%
  select(-LOC_OF_OCCUR_DESC, -LOC_CLASSFCTN_DESC)
colnames(NYPD_shooting)
```

I also want to take a look at the next largest missing data column which is `LOCATION_DESC`.

```{r show_location_desc, message=FALSE}
unique(NYPD_shooting$LOCATION_DESC)
```

This column shows the specific location that the shooting takes place which can be useful. The problem is the missing data. I think I will leave it in case we want to look more into that specifically. The same can be said about `PERP_AGE_GROUP`, `PERP_SEX`, and `PERP_RACE`. Optionally, we can replace the missing rows with something like `Unknown` to make it easier to see on charts.

## Plotting Data and Analysis

```{r plot_shotting_per_year, message=FALSE}
NYPD_shooting %>%
  mutate(year = year(mdy(OCCUR_DATE))) %>%
  count(year) %>%
  ggplot(aes(x = year, y = n)) +
  geom_line() +
  geom_point() +
  labs(
  title = "NYPD Shooting Incidents per Year",
    x = "Year",
    y = "Number of Shootings"
  )
```

This graph shows the number of shooting incidents by year. What's interesting about this chart is that the number of shooting seems to go down over the years and spikes in 2020. What I find weird is that 2020 is around the Covid-19 incident and we were in lock down.

```{r plot_shootings_per_location, message=FALSE}
NYPD_shooting %>%
  count(BORO) %>%
  ggplot(aes(x = BORO, y = n)) +
  geom_col() +
  labs(
    title = "NYPD Shooting Incidents per Location",
    x = "Location",
    y = "Number of Shootings"
  )
```

This graph shows the amount of shooting incidents per location. We can see that there are more shooting incidents in Bronx and Brooklyn we also have to keep in mind that this is not per a certain amount of people so the data can be more skewed depending on the population density.

For this next model, I want my prediction `y` to be `STATISTICAL_MURDER_FLAG` and my predictors will be:

-   `OCCUR_TIME`
-   `BORO`
-   `PERP_SEX`
-   `PERP_AGE_GROUP`
-   `PERP_RACE`

To do this, I first remove all the null columns.

```{r remove_na, message=FALSE}
NYPD_shooting_clean <- NYPD_shooting %>% drop_na()
colSums(is.na(NYPD_shooting_clean))
```

### Model

I want to see if the time of day and location matter for the murder rate. For this, I will use a GLM model

```{r model_murder_flag, message=FALSE}
model <- glm(STATISTICAL_MURDER_FLAG ~ hour(OCCUR_TIME) + BORO, 
             data=NYPD_shooting_clean,
             family="binomial")
summary(model)
```

From the p-value of `hour(OCCUR_TIME)`, the time doesn't seem to make a big difference to the murder chance.

```{r model_visual, message=FALSE}
ggplot(NYPD_shooting_clean, aes(hour(OCCUR_TIME))) + geom_bar(aes(fill=STATISTICAL_MURDER_FLAG)) +
  labs(
    title="Shooting Incident by Hour split by Murder Rate",
    x = "Hour",
    y = "Incidents"
  )
```

Visually we see that although we do see a trend of less shootings in the early hours of the day, the `TRUE` and `FALSE` are proportional at each hour. Conclusively it seems as though the murder rate doesn't change by time but by the number of incidents.

## Conclusion

Throughout this report, I try to clean the data by removing unused columns and to combine columns that I see fit. I also try to present the data to gain some insight on the amount of shootings per location and per year. I also tried to fit the model with the hour of data and murder to see if murder rate to see if there is a significance.

### Bias

A big bias that someone might be wary of is to be too cautious of unintentionally reinforcing stereotypes or misrepresenting groups. Because of this, one might avoid using the columns:

-   `perp_sex`
-   `vic_sex`
-   `perp_race`
-   `vic_race`

Data gathering is also a big bias. There could be shootings that go undocumented since this data set is only of those that we know of. There are also a lot of empty or null fields. This data set also includes data from when we got the Covid-19 lock down which is not mention here.
